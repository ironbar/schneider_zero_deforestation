{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Schneider's Zero Deforestation Hackaton Predict deforestation on satellite images https://nuwe.io/dev/challenges/data-science-se-european Docs Enjoy the documentation of the project created with MkDocs at https://ironbar.github.io/schneider_zero_deforestation/ Solution As required by the challenge the solution has 3 files: main.sh predictions.json presentation.pdf Solution summary An ensemble of the best 13 fine-tuned models comprising of: OpenClip models pretrained on LAION dataset (ViT-B-16-plus-240, ViT-B-32, ViT-L-14, ViT-H-14, ViT-g-14, ViT-B-16) Keras models pretrained on Imagenet (ResNet50, ResNet50V2, MobileNetV2) Achieves a validation f1 score of 0.754 Code structure |_ docs: documents made during the challenge according to CRISP-DM methodology |_ models: predictions and models trained for the challenge |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"Schneider's Zero Deforestation Hackaton"},{"location":"#schneiders-zero-deforestation-hackaton","text":"Predict deforestation on satellite images https://nuwe.io/dev/challenges/data-science-se-european","title":"Schneider's Zero Deforestation Hackaton"},{"location":"#docs","text":"Enjoy the documentation of the project created with MkDocs at https://ironbar.github.io/schneider_zero_deforestation/","title":"Docs"},{"location":"#solution","text":"As required by the challenge the solution has 3 files: main.sh predictions.json presentation.pdf","title":"Solution"},{"location":"#solution-summary","text":"An ensemble of the best 13 fine-tuned models comprising of: OpenClip models pretrained on LAION dataset (ViT-B-16-plus-240, ViT-B-32, ViT-L-14, ViT-H-14, ViT-g-14, ViT-B-16) Keras models pretrained on Imagenet (ResNet50, ResNet50V2, MobileNetV2) Achieves a validation f1 score of 0.754","title":"Solution summary"},{"location":"#code-structure","text":"|_ docs: documents made during the challenge according to CRISP-DM methodology |_ models: predictions and models trained for the challenge |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"Code structure"},{"location":"01_Business_Understanding/","text":"Business Understanding Challenge description The data science challenge will consist in creating an image classification model, which from a given dataset, predicts what type of deforestation appears in the image with the objective of early detection of this type of actions in protected lands. few thousand training and testing images The challenge will be to create an image classification model and train it with the training images we provide, once you find the model that maximizes the f1-score, you will have to apply your model to classify the testing images. Your score will depend on the F1-score of your model with the test images, the quality of your code and the explanation of the solution you provide. the dataset will consist of a few thousand satellite images of forests. And a document that relates these images to their respective label. The number of classes will be revealed on the day of the competition. Evaluation Your score will depend on: the F1-score of your model with the test images the quality of your code the quality of the explanation of the solution you provide. So we have to train a precise model, but at the same time we have to make clean code and nice documentation. The winning teams will be determined by a combination of hard skills and soft skills correction. The hard skills will be the responsibility of NUWE and the soft skills will be the responsibility of Schneider Electric. It is unclear how Schneider will judge this soft skills. On the challenge there are some times to interact with the Schneider team so maybe they will use that information. However this mention to the soft skills is only done in the rules, not in the website of the challenge. F1-score F-score on Wikipedia The F1 score gives the same weight to precision and recall. This implies that we should use a balanced dataset when training to be able to maximize F1 score. For example if the train dataset has too many positive classes the model will tend to focus on recall at the cost is precision because the cost of misclassifying a positive class is bigger than misclassifying a a negative class. F1-score implementations sklearn keras In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter. Assess situation Timeline The challenge starts ad 09:00 on November 19th 2022. It ends on 23:59 of the same day. There is some mentoring time along the day. So excluding mentoring time, lunch time, ... there are around 10 hours to do the challenge. The timeline is really short compared to typical challenges. Probably the key to do well in this challenge is the preparation. There is already information about the challenge and it is possible to read about the state of the art and do some preliminary work. Terminology Project Plan The challenge is to make a model to classify satellite images about forest deforestation. The number of available images is in the order of thousands, so it is a small dataset. When dealing with small datasets the typical solution is to take a pretrained model and fine-tune it on the small dataset. So that is likely the winning strategy for this challenge: finding good pretrained models and fine-tuning them. However satellite images sometimes have frequency bands different to the typical RGB images and that would make finding a pretrained model more difficult. Stable-diffusion is able to generate satellite images of deforestation forest thus it is likely that Laion OpenClip could be helpful for this challenge. The plan would be: On the days before the challenge gather information about pretrained models and datasets that could be useful to classify satellite images of forests. If possible do a fine-tuning rehearsal with them so the day of the challenge the work is almost done. On the day of the challenge explore the dataset to decide what of the available pretrained models are the most useful. Using cross-validation or a fixed validation set fine-tune an ensemble of models for the challenge. Create very good documentation of the experimentation process.","title":"Business Understanding"},{"location":"01_Business_Understanding/#business-understanding","text":"","title":"Business Understanding"},{"location":"01_Business_Understanding/#challenge-description","text":"The data science challenge will consist in creating an image classification model, which from a given dataset, predicts what type of deforestation appears in the image with the objective of early detection of this type of actions in protected lands. few thousand training and testing images The challenge will be to create an image classification model and train it with the training images we provide, once you find the model that maximizes the f1-score, you will have to apply your model to classify the testing images. Your score will depend on the F1-score of your model with the test images, the quality of your code and the explanation of the solution you provide. the dataset will consist of a few thousand satellite images of forests. And a document that relates these images to their respective label. The number of classes will be revealed on the day of the competition.","title":"Challenge description"},{"location":"01_Business_Understanding/#evaluation","text":"Your score will depend on: the F1-score of your model with the test images the quality of your code the quality of the explanation of the solution you provide. So we have to train a precise model, but at the same time we have to make clean code and nice documentation. The winning teams will be determined by a combination of hard skills and soft skills correction. The hard skills will be the responsibility of NUWE and the soft skills will be the responsibility of Schneider Electric. It is unclear how Schneider will judge this soft skills. On the challenge there are some times to interact with the Schneider team so maybe they will use that information. However this mention to the soft skills is only done in the rules, not in the website of the challenge.","title":"Evaluation"},{"location":"01_Business_Understanding/#f1-score","text":"F-score on Wikipedia The F1 score gives the same weight to precision and recall. This implies that we should use a balanced dataset when training to be able to maximize F1 score. For example if the train dataset has too many positive classes the model will tend to focus on recall at the cost is precision because the cost of misclassifying a positive class is bigger than misclassifying a a negative class.","title":"F1-score"},{"location":"01_Business_Understanding/#f1-score-implementations","text":"sklearn keras In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter.","title":"F1-score implementations"},{"location":"01_Business_Understanding/#assess-situation","text":"","title":"Assess situation"},{"location":"01_Business_Understanding/#timeline","text":"The challenge starts ad 09:00 on November 19th 2022. It ends on 23:59 of the same day. There is some mentoring time along the day. So excluding mentoring time, lunch time, ... there are around 10 hours to do the challenge. The timeline is really short compared to typical challenges. Probably the key to do well in this challenge is the preparation. There is already information about the challenge and it is possible to read about the state of the art and do some preliminary work.","title":"Timeline"},{"location":"01_Business_Understanding/#terminology","text":"","title":"Terminology"},{"location":"01_Business_Understanding/#project-plan","text":"The challenge is to make a model to classify satellite images about forest deforestation. The number of available images is in the order of thousands, so it is a small dataset. When dealing with small datasets the typical solution is to take a pretrained model and fine-tune it on the small dataset. So that is likely the winning strategy for this challenge: finding good pretrained models and fine-tuning them. However satellite images sometimes have frequency bands different to the typical RGB images and that would make finding a pretrained model more difficult. Stable-diffusion is able to generate satellite images of deforestation forest thus it is likely that Laion OpenClip could be helpful for this challenge. The plan would be: On the days before the challenge gather information about pretrained models and datasets that could be useful to classify satellite images of forests. If possible do a fine-tuning rehearsal with them so the day of the challenge the work is almost done. On the day of the challenge explore the dataset to decide what of the available pretrained models are the most useful. Using cross-validation or a fixed validation set fine-tune an ensemble of models for the challenge. Create very good documentation of the experimentation process.","title":"Project Plan"},{"location":"02_Data_Understanding/","text":"Data Understanding Collect initial data Downloading the data was very easy because it is a small dataset. External data I'm not going to use External data for this challenge. Describe data For this challenge, you will have 2 CSVs: Train and Test. As their names indicate, the first one will be used to train your classification model on the forest images and test to know to which label they belong. It is important to remember to be careful with the paths for reading images in the train_test_data folder. There are 3 categories: Plantation :Encoded with number 0, Network of rectangular plantation blocks, connected by a well-defined road grid. In hilly areas the layout of the plantation may follow topographic features. In this group you can find: Oil Palm Plantation, Timber Plantation and Other large-scale plantations. Grassland/Shrubland : Encoded with number 1, Large homogeneous areas with few or sparse shrubs or trees, and which are generally persistent. Distinguished by the absence of signs of agriculture, such as clearly defined field boundaries. Smallholder Agriculture : Encoded with number 2, Small scale area, in which you can find deforestation covered by agriculture, mixed plantation or oil palm plantation. Explore data We have 1714 images for train-val and 635 test images. This is a very small dataset. They provide coordinates and year for the pictures. However that information in theory should not be relevant since we have the images. All the information to classify the deforestation categories should be available in the images There are 3 categories and they are not balanced. So it is possible that balancing them during the training will result on a better validation F1 score. All images are squared and have the same shape 332x332x3. They are saved in png format. Verify data quality The images are very dark, maybe a preprocessing step to increase the contrast could be useful. I have studied the distribution of the pixel values and found that we could multiply the images by 2 to increase the contrast. The following plot shows a few original images. Now the same images after increasing the contrast with a factor of 2 are shown. Amount of data The dataset is very tiny for the Deep Learning standards. The best option is to fine-tune a model that was pretrained on a bigger dataset.","title":"Data Understanding"},{"location":"02_Data_Understanding/#data-understanding","text":"","title":"Data Understanding"},{"location":"02_Data_Understanding/#collect-initial-data","text":"Downloading the data was very easy because it is a small dataset.","title":"Collect initial data"},{"location":"02_Data_Understanding/#external-data","text":"I'm not going to use External data for this challenge.","title":"External data"},{"location":"02_Data_Understanding/#describe-data","text":"For this challenge, you will have 2 CSVs: Train and Test. As their names indicate, the first one will be used to train your classification model on the forest images and test to know to which label they belong. It is important to remember to be careful with the paths for reading images in the train_test_data folder. There are 3 categories: Plantation :Encoded with number 0, Network of rectangular plantation blocks, connected by a well-defined road grid. In hilly areas the layout of the plantation may follow topographic features. In this group you can find: Oil Palm Plantation, Timber Plantation and Other large-scale plantations. Grassland/Shrubland : Encoded with number 1, Large homogeneous areas with few or sparse shrubs or trees, and which are generally persistent. Distinguished by the absence of signs of agriculture, such as clearly defined field boundaries. Smallholder Agriculture : Encoded with number 2, Small scale area, in which you can find deforestation covered by agriculture, mixed plantation or oil palm plantation.","title":"Describe data"},{"location":"02_Data_Understanding/#explore-data","text":"We have 1714 images for train-val and 635 test images. This is a very small dataset. They provide coordinates and year for the pictures. However that information in theory should not be relevant since we have the images. All the information to classify the deforestation categories should be available in the images There are 3 categories and they are not balanced. So it is possible that balancing them during the training will result on a better validation F1 score. All images are squared and have the same shape 332x332x3. They are saved in png format.","title":"Explore data"},{"location":"02_Data_Understanding/#verify-data-quality","text":"The images are very dark, maybe a preprocessing step to increase the contrast could be useful. I have studied the distribution of the pixel values and found that we could multiply the images by 2 to increase the contrast. The following plot shows a few original images. Now the same images after increasing the contrast with a factor of 2 are shown.","title":"Verify data quality"},{"location":"02_Data_Understanding/#amount-of-data","text":"The dataset is very tiny for the Deep Learning standards. The best option is to fine-tune a model that was pretrained on a bigger dataset.","title":"Amount of data"},{"location":"03_Data_Preparation/","text":"Data Preparation Select Data All the available data will be used for this challenge. Clean Data There is no time to clean the data. Since the dataset is very small I will assume that the data is clean. Construct Data I will only use the images for training the model. Since my plan is to use different pretrained models I will have to do different preprocessing for each of them.","title":"Data Preparation"},{"location":"03_Data_Preparation/#data-preparation","text":"","title":"Data Preparation"},{"location":"03_Data_Preparation/#select-data","text":"All the available data will be used for this challenge.","title":"Select Data"},{"location":"03_Data_Preparation/#clean-data","text":"There is no time to clean the data. Since the dataset is very small I will assume that the data is clean.","title":"Clean Data"},{"location":"03_Data_Preparation/#construct-data","text":"I will only use the images for training the model. Since my plan is to use different pretrained models I will have to do different preprocessing for each of them.","title":"Construct Data"},{"location":"05_Solution_Summary/","text":"Solution Summary Solution summary","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-summary","text":"","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-summary_1","text":"","title":"Solution summary"},{"location":"06_Winning_Model_Documentation/","text":"Winning model documentation Winning Model Documentation Guidelines A. MODEL SUMMARY A1. Background on you/your team Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com A2. Background on you/your team What is your academic/professional background? Did you have any prior experience that helped you succeed in this competition? What made you decide to enter this competition? How much time did you spend on the competition? If part of a team, how did you decide to team up? If you competed as part of a team, who did what? A3. Summary A4. Features Selection / Engineering What were the most important features? How did you select features? Did you make any important feature transformations? Did you find any interesting interactions between features? Did you use external data? (if permitted) A5. Training Method(s) What training methods did you use? Did you ensemble the models? If you did ensemble, how did you weight the different models? A6. Interesting findings What was the most important trick you used? What do you think set you apart from others in the competition? Did you find any interesting relationships in the data that don't fit in the sections above? A7. Simple Features and Methods A8. Model Execution Time How long does it take to train your model? How long does it take to generate predictions using your model? How long does it take to train the simplified model (referenced in section A6)? How long does it take to generate predictions from the simplified model? A9. References B. SUBMISSION MODEL B1. All code, data, and your trained model goes in a single archive B2. README.md B3. Configuration files B4. requirements.txt B5. directory_structure.txt B6. SETTINGS.json B7. Serialized copy of the trained model B8. entry_points.md","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#winning-model-documentation","text":"Winning Model Documentation Guidelines","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#a-model-summary","text":"","title":"A. MODEL SUMMARY"},{"location":"06_Winning_Model_Documentation/#a1-background-on-youyour-team","text":"Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com","title":"A1. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#a2-background-on-youyour-team","text":"","title":"A2. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#what-is-your-academicprofessional-background","text":"","title":"What is your academic/professional background?"},{"location":"06_Winning_Model_Documentation/#did-you-have-any-prior-experience-that-helped-you-succeed-in-this-competition","text":"","title":"Did you have any prior experience that helped you succeed in this competition?"},{"location":"06_Winning_Model_Documentation/#what-made-you-decide-to-enter-this-competition","text":"","title":"What made you decide to enter this competition?"},{"location":"06_Winning_Model_Documentation/#how-much-time-did-you-spend-on-the-competition","text":"","title":"How much time did you spend on the competition?"},{"location":"06_Winning_Model_Documentation/#if-part-of-a-team-how-did-you-decide-to-team-up","text":"","title":"If part of a team, how did you decide to team up?"},{"location":"06_Winning_Model_Documentation/#if-you-competed-as-part-of-a-team-who-did-what","text":"","title":"If you competed as part of a team, who did what?"},{"location":"06_Winning_Model_Documentation/#a3-summary","text":"","title":"A3. Summary"},{"location":"06_Winning_Model_Documentation/#a4-features-selection-engineering","text":"","title":"A4. Features Selection / Engineering"},{"location":"06_Winning_Model_Documentation/#what-were-the-most-important-features","text":"","title":"What were the most important features?"},{"location":"06_Winning_Model_Documentation/#how-did-you-select-features","text":"","title":"How did you select features?"},{"location":"06_Winning_Model_Documentation/#did-you-make-any-important-feature-transformations","text":"","title":"Did you make any important feature transformations?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-interactions-between-features","text":"","title":"Did you find any interesting interactions between features?"},{"location":"06_Winning_Model_Documentation/#did-you-use-external-data-if-permitted","text":"","title":"Did you use external data? (if permitted)"},{"location":"06_Winning_Model_Documentation/#a5-training-methods","text":"","title":"A5. Training Method(s)"},{"location":"06_Winning_Model_Documentation/#what-training-methods-did-you-use","text":"","title":"What training methods did you use?"},{"location":"06_Winning_Model_Documentation/#did-you-ensemble-the-models","text":"","title":"Did you ensemble the models?"},{"location":"06_Winning_Model_Documentation/#if-you-did-ensemble-how-did-you-weight-the-different-models","text":"","title":"If you did ensemble, how did you weight the different models?"},{"location":"06_Winning_Model_Documentation/#a6-interesting-findings","text":"","title":"A6. Interesting findings"},{"location":"06_Winning_Model_Documentation/#what-was-the-most-important-trick-you-used","text":"","title":"What was the most important trick you used?"},{"location":"06_Winning_Model_Documentation/#what-do-you-think-set-you-apart-from-others-in-the-competition","text":"","title":"What do you think set you apart from others in the competition?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-relationships-in-the-data-that-dont-fit-in-the-sections-above","text":"","title":"Did you find any interesting relationships in the data that don't fit in the sections above?"},{"location":"06_Winning_Model_Documentation/#a7-simple-features-and-methods","text":"","title":"A7. Simple Features and Methods"},{"location":"06_Winning_Model_Documentation/#a8-model-execution-time","text":"","title":"A8. Model Execution Time"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-your-model","text":"","title":"How long does it take to train your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-using-your-model","text":"","title":"How long does it take to generate predictions using your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-the-simplified-model-referenced-in-section-a6","text":"","title":"How long does it take to train the simplified model (referenced in section A6)?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-from-the-simplified-model","text":"","title":"How long does it take to generate predictions from the simplified model?"},{"location":"06_Winning_Model_Documentation/#a9-references","text":"","title":"A9. References"},{"location":"06_Winning_Model_Documentation/#b-submission-model","text":"","title":"B. SUBMISSION MODEL"},{"location":"06_Winning_Model_Documentation/#b1-all-code-data-and-your-trained-model-goes-in-a-single-archive","text":"","title":"B1. All code, data, and your trained model goes in a single archive"},{"location":"06_Winning_Model_Documentation/#b2-readmemd","text":"","title":"B2. README.md"},{"location":"06_Winning_Model_Documentation/#b3-configuration-files","text":"","title":"B3. Configuration files"},{"location":"06_Winning_Model_Documentation/#b4-requirementstxt","text":"","title":"B4. requirements.txt"},{"location":"06_Winning_Model_Documentation/#b5-directory_structuretxt","text":"","title":"B5. directory_structure.txt"},{"location":"06_Winning_Model_Documentation/#b6-settingsjson","text":"","title":"B6. SETTINGS.json"},{"location":"06_Winning_Model_Documentation/#b7-serialized-copy-of-the-trained-model","text":"","title":"B7. Serialized copy of the trained model"},{"location":"06_Winning_Model_Documentation/#b8-entry_pointsmd","text":"","title":"B8. entry_points.md"},{"location":"modeling/","text":"Modeling Select modeling technique My plan for the challenge is to fine-tune several pretrained models for the task of satellite image classification. Then I will build an ensemble using those models. In this section I'm going to gather information about the possible models that we could use. LAION OpenClip To my knowledge this is the biggest pretrained model that is available today. It has a github repo where it is shown how to use the model for zero-shot classification and another repo where it shows one way to fine-tune I should try the fine-tuning but I could also try to train a logistic regression model on top of OpenClip. There are multiple versions of OpenClip that I could try for the ensemble Models pretrained on Imagenet Detecting deforestation from satellite images In this post they use a simple ResNet50 and get similar results to the Kaggle competition. They do not say it explicitly but it is very likely the ResNet was pretrained on Imagenet. On the Kaggle competition all the teams used pretrained models: overfitting describing its models Multi-Label Classification of Satellite Photos of the Amazon Rainforest Here also pretrained models on Imagenet are used. Keras models pretrained on Imagenet Big Transfer (BiT) https://github.com/google-research/big_transfer In this repository we release multiple models from the Big Transfer (BiT): General Visual Representation Learning paper that were pre-trained on the ILSVRC-2012 and ImageNet-21k datasets. We provide the code to fine-tuning the released models in the major deep learning frameworks TensorFlow 2, PyTorch and Jax/Flax. We hope that the computer vision community will benefit by employing more powerful ImageNet-21k pretrained models as opposed to conventional models pre-trained on the ILSVRC-2012 dataset. I have already used this models for other competitions. I should have a look at the fine-tuning code and test it if possible before the challenge. Specific satellite image pretrained models Let's try to find models that were trained on satellite image data. Tensorflow Hub Remote sensing This collection contains models that were pre-trained for the remote sensing domain concentrating on satellite and airborne imagery. These models provide representations for transfer learning on custom datasets. All parameters in the modules are trainable, and fine-tuning all parameters is the recommended practice. Did not find anything relevant on HuggingFace An Empirical Study of Remote Sensing Pretraining This pytorch models are pretrained on MillionAID. Million-AID is a large-scale benchmark dataset containing a million instances for remote sensing scene classification. However this pretraining does not seem to give an advantage over Imagenet pretraining. Useful resources https://github.com/robmarkcole/satellite-image-deep-learning This repository lists resources on the topic of deep learning applied to satellite and aerial imagery. Kaggle's Planet: Understanding the Amazon from Space Summary If the data for the channel are standard RGB images we have 4 different families of pretrained models to create a powerful ensemble: Generate test design Since there is a very limited timeline I believe that the best validation strategy is to simply split the train set between a train and a validation set. Using cross-validation will likely result on slightly better scores but will require more computing time. Once we have the data I will explore it to see if a random split is enough or a more specific criteria is needed. Also the number of submissions and to see if there is public and private test set will be relevant to choose the test design.","title":"Modeling"},{"location":"modeling/#modeling","text":"","title":"Modeling"},{"location":"modeling/#select-modeling-technique","text":"My plan for the challenge is to fine-tune several pretrained models for the task of satellite image classification. Then I will build an ensemble using those models. In this section I'm going to gather information about the possible models that we could use.","title":"Select modeling technique"},{"location":"modeling/#laion-openclip","text":"To my knowledge this is the biggest pretrained model that is available today. It has a github repo where it is shown how to use the model for zero-shot classification and another repo where it shows one way to fine-tune I should try the fine-tuning but I could also try to train a logistic regression model on top of OpenClip. There are multiple versions of OpenClip that I could try for the ensemble","title":"LAION OpenClip"},{"location":"modeling/#models-pretrained-on-imagenet","text":"Detecting deforestation from satellite images In this post they use a simple ResNet50 and get similar results to the Kaggle competition. They do not say it explicitly but it is very likely the ResNet was pretrained on Imagenet. On the Kaggle competition all the teams used pretrained models: overfitting describing its models Multi-Label Classification of Satellite Photos of the Amazon Rainforest Here also pretrained models on Imagenet are used. Keras models pretrained on Imagenet","title":"Models pretrained on Imagenet"},{"location":"modeling/#big-transfer-bit","text":"https://github.com/google-research/big_transfer In this repository we release multiple models from the Big Transfer (BiT): General Visual Representation Learning paper that were pre-trained on the ILSVRC-2012 and ImageNet-21k datasets. We provide the code to fine-tuning the released models in the major deep learning frameworks TensorFlow 2, PyTorch and Jax/Flax. We hope that the computer vision community will benefit by employing more powerful ImageNet-21k pretrained models as opposed to conventional models pre-trained on the ILSVRC-2012 dataset. I have already used this models for other competitions. I should have a look at the fine-tuning code and test it if possible before the challenge.","title":"Big Transfer (BiT)"},{"location":"modeling/#specific-satellite-image-pretrained-models","text":"Let's try to find models that were trained on satellite image data. Tensorflow Hub Remote sensing This collection contains models that were pre-trained for the remote sensing domain concentrating on satellite and airborne imagery. These models provide representations for transfer learning on custom datasets. All parameters in the modules are trainable, and fine-tuning all parameters is the recommended practice. Did not find anything relevant on HuggingFace An Empirical Study of Remote Sensing Pretraining This pytorch models are pretrained on MillionAID. Million-AID is a large-scale benchmark dataset containing a million instances for remote sensing scene classification. However this pretraining does not seem to give an advantage over Imagenet pretraining.","title":"Specific satellite image pretrained models"},{"location":"modeling/#useful-resources","text":"https://github.com/robmarkcole/satellite-image-deep-learning This repository lists resources on the topic of deep learning applied to satellite and aerial imagery. Kaggle's Planet: Understanding the Amazon from Space","title":"Useful resources"},{"location":"modeling/#summary","text":"If the data for the channel are standard RGB images we have 4 different families of pretrained models to create a powerful ensemble:","title":"Summary"},{"location":"modeling/#generate-test-design","text":"Since there is a very limited timeline I believe that the best validation strategy is to simply split the train set between a train and a validation set. Using cross-validation will likely result on slightly better scores but will require more computing time. Once we have the data I will explore it to see if a random split is enough or a more specific criteria is needed. Also the number of submissions and to see if there is public and private test set will be relevant to choose the test design.","title":"Generate test design"},{"location":"modeling/01_Iteration_1/","text":"Iteration 1. Fine-tune ResNet50 pretrained on Imagenet 1.1 Goal The goal of this Iteration is to fine-tune a ResNet50 Keras pretrained model that was trained on Imagenet. 1.2 Development 1.2.1 Install tensorflow with pip https://www.tensorflow.org/install/pip conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/ pip install tensorflow==2.10 python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" I had to delete and create the environment from zero because tensorflow is not ready for python 3.11. Also I had to downgrade tensorflow from 2.11 to 2.10 because GPU was not working. Then I had to downgrade again to 2.8.3 because of this bug 1.2.2 Install other dependencies pip install opencv-python pip install tensorflow-addons ResNet50 train script I have the following questions regarding the fine-tuning strategy: Does contrast increase improve the results? Dropout Data augmentation Class balance The best way to answer them is to create an script and run multiple experiments for each option. That way we could have mean value and uncertainty for each option. 1.3 Results I have run 6 experiments for each option to be able to estimate the mean f1 score and the uncertainty. All results are validation scores. I'm using 20% of the training data for validation, always with the same seed. 1.3.1 Does contrast increase improve the results? experiment mean f1 score uncertainty original images 0.715 0.006 increase contrast x2 0.709 0.014 There is no statistically significant difference between the two experiments. Thus it's better to keep it simple and use the original images. 1.3.2 Data augmentation random flip random rotation random contrast random translation mean f1 score uncertainty - - - - 0.700 0.006 yes - - - 0.713 0.006 yes 54\u00ba - - 0.705 0.010 yes 30\u00ba - - 0.717 0.004 yes 30\u00ba yes - 0.702 0.009 yes 30\u00ba - yes 0.691 0.009 The best data augmentation configuration appears to be to use random flips and random rotations. In some cases the differences are not significative but that configuration is the one that gets the higher mean f1 score. 1.3.3 Dropout dropout mean f1 score uncertainty 0 0.701 0.013 0.1 0.71 0.009 0.2 0.706 0.01 0.5 0.709 0.013 The table above shows that there is too much uncertainty on the experiments. I repeated the baseline from the best configuration of data augmentation with much worse results as it is shown in the row with no dropout. There are no significative differences between all the experiments with dropout. Thus I won't be using it. 1.3.4 Class balance On this experiments I have set the average of the f1 score to macro because it has been said on discord. Thus I have repeated the baseline results with the macro average. Since the 3 categories are unbalanced I'm going to create a generator to balance them and see how that affects to the validation score. experiment mean f1 score uncertainty unbalanced dataset 0.709 0.008 balanced generator 0.710 0.007 There is no statistically significant difference between the two experiments. For simplicity I will be training on the unbalanced dataset. Also there was no big difference with previous experiments that were not using f1 macro average. 1.4 Summary We have tuned the training configuration for fine-tuning ResNet50 model in the challenge's data. 1.5 Next steps Extend the fine-tuning to other architectures.","title":"Iteration 1. Fine-tune ResNet50 pretrained on Imagenet"},{"location":"modeling/01_Iteration_1/#iteration-1-fine-tune-resnet50-pretrained-on-imagenet","text":"","title":"Iteration 1. Fine-tune ResNet50 pretrained on Imagenet"},{"location":"modeling/01_Iteration_1/#11-goal","text":"The goal of this Iteration is to fine-tune a ResNet50 Keras pretrained model that was trained on Imagenet.","title":"1.1 Goal"},{"location":"modeling/01_Iteration_1/#12-development","text":"","title":"1.2 Development"},{"location":"modeling/01_Iteration_1/#121-install-tensorflow-with-pip","text":"https://www.tensorflow.org/install/pip conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/ pip install tensorflow==2.10 python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" I had to delete and create the environment from zero because tensorflow is not ready for python 3.11. Also I had to downgrade tensorflow from 2.11 to 2.10 because GPU was not working. Then I had to downgrade again to 2.8.3 because of this bug","title":"1.2.1 Install tensorflow with pip"},{"location":"modeling/01_Iteration_1/#122-install-other-dependencies","text":"pip install opencv-python pip install tensorflow-addons","title":"1.2.2 Install other dependencies"},{"location":"modeling/01_Iteration_1/#resnet50-train-script","text":"I have the following questions regarding the fine-tuning strategy: Does contrast increase improve the results? Dropout Data augmentation Class balance The best way to answer them is to create an script and run multiple experiments for each option. That way we could have mean value and uncertainty for each option.","title":"ResNet50 train script"},{"location":"modeling/01_Iteration_1/#13-results","text":"I have run 6 experiments for each option to be able to estimate the mean f1 score and the uncertainty. All results are validation scores. I'm using 20% of the training data for validation, always with the same seed.","title":"1.3 Results"},{"location":"modeling/01_Iteration_1/#131-does-contrast-increase-improve-the-results","text":"experiment mean f1 score uncertainty original images 0.715 0.006 increase contrast x2 0.709 0.014 There is no statistically significant difference between the two experiments. Thus it's better to keep it simple and use the original images.","title":"1.3.1 Does contrast increase improve the results?"},{"location":"modeling/01_Iteration_1/#132-data-augmentation","text":"random flip random rotation random contrast random translation mean f1 score uncertainty - - - - 0.700 0.006 yes - - - 0.713 0.006 yes 54\u00ba - - 0.705 0.010 yes 30\u00ba - - 0.717 0.004 yes 30\u00ba yes - 0.702 0.009 yes 30\u00ba - yes 0.691 0.009 The best data augmentation configuration appears to be to use random flips and random rotations. In some cases the differences are not significative but that configuration is the one that gets the higher mean f1 score.","title":"1.3.2 Data augmentation"},{"location":"modeling/01_Iteration_1/#133-dropout","text":"dropout mean f1 score uncertainty 0 0.701 0.013 0.1 0.71 0.009 0.2 0.706 0.01 0.5 0.709 0.013 The table above shows that there is too much uncertainty on the experiments. I repeated the baseline from the best configuration of data augmentation with much worse results as it is shown in the row with no dropout. There are no significative differences between all the experiments with dropout. Thus I won't be using it.","title":"1.3.3 Dropout"},{"location":"modeling/01_Iteration_1/#134-class-balance","text":"On this experiments I have set the average of the f1 score to macro because it has been said on discord. Thus I have repeated the baseline results with the macro average. Since the 3 categories are unbalanced I'm going to create a generator to balance them and see how that affects to the validation score. experiment mean f1 score uncertainty unbalanced dataset 0.709 0.008 balanced generator 0.710 0.007 There is no statistically significant difference between the two experiments. For simplicity I will be training on the unbalanced dataset. Also there was no big difference with previous experiments that were not using f1 macro average.","title":"1.3.4 Class balance"},{"location":"modeling/01_Iteration_1/#14-summary","text":"We have tuned the training configuration for fine-tuning ResNet50 model in the challenge's data.","title":"1.4 Summary"},{"location":"modeling/01_Iteration_1/#15-next-steps","text":"Extend the fine-tuning to other architectures.","title":"1.5 Next steps"},{"location":"modeling/02_Iteration_2/","text":"Iteration 2. Fine-tune multiple models 2.1 Goal We are going to continue with the work from the previous iteration and fine-tune multiple different architectures so we can later create an ensemble. 2.2 Development Keras provides multiple pretrained models along with a function to preprocess the images. I can extend the code that I had for ResNet50 to work with other architectures with minimal changes. All keras pretrained models 2.3 Results Architectures val f1 score uncertainty ResNet50 0.7169 0.0124 MobileNetV2 0.6954 0.0197 Xception 0.7019 0.0147 ResNet50V2 0.7185 0.0251 EfficientNetV2B0 0.6894 0.0164 EfficientNetV2B3 0.6913 0.0036 2.4 Summary We have trained different architecture that we could ensemble later. 2.5 Next steps Try OpenClip models.","title":"Iteration 2. Fine-tune multiple models"},{"location":"modeling/02_Iteration_2/#iteration-2-fine-tune-multiple-models","text":"","title":"Iteration 2. Fine-tune multiple models"},{"location":"modeling/02_Iteration_2/#21-goal","text":"We are going to continue with the work from the previous iteration and fine-tune multiple different architectures so we can later create an ensemble.","title":"2.1 Goal"},{"location":"modeling/02_Iteration_2/#22-development","text":"Keras provides multiple pretrained models along with a function to preprocess the images. I can extend the code that I had for ResNet50 to work with other architectures with minimal changes. All keras pretrained models","title":"2.2 Development"},{"location":"modeling/02_Iteration_2/#23-results","text":"Architectures val f1 score uncertainty ResNet50 0.7169 0.0124 MobileNetV2 0.6954 0.0197 Xception 0.7019 0.0147 ResNet50V2 0.7185 0.0251 EfficientNetV2B0 0.6894 0.0164 EfficientNetV2B3 0.6913 0.0036","title":"2.3 Results"},{"location":"modeling/02_Iteration_2/#24-summary","text":"We have trained different architecture that we could ensemble later.","title":"2.4 Summary"},{"location":"modeling/02_Iteration_2/#25-next-steps","text":"Try OpenClip models.","title":"2.5 Next steps"},{"location":"modeling/03_Iteration_3/","text":"Iteration 3. Train Logistic Regression on top of OpenClip 3.1 Goal We are going to train a Logistic Regression model on top of the OpenClip embeddings. OpenClip models are pretrained on LAION dataset which is the same dataset that was used to train Stable Diffusion. Stable diffusion is able to generate satellite images so that implies that there are satellite images on LAION dataset. Thus we can use the OpenClip embeddings to train a model that can classify satellite images. 3.2 Development I have prepared a script following the examples of the github repo If we had more time it would have been better to fine-tune the OpenClip model instead of training a Logistic Regression on top of the embeddings. 3.3 Results model pretrained val f1 score ViT-B-16-plus-240 laion400m_e32 0.7837 ViT-B-32-quickgelu laion2b_s34b_b79k 0.743 ViT-L-14 laion2b_s32b_b82k 0.7335 ViT-L-14 laion400m_e32 0.7325 ViT-H-14 laion2b_s32b_b79k 0.7248 ViT-B-16 openai 0.7245 ViT-g-14 laion2b_s12b_b42k 0.7224 ViT-B-16 laion400m_e32 0.7193 ViT-B-32-quickgelu openai 0.715 ViT-L-14 openai 0.7084 ViT-B-32-quickgelu laion400m_e32 0.6966 The results are quite impressive for a simple Logistic Regression model, this implies that the features extracted by the OpenClip models are very useful for the task. Specially good is the result of ViT-B-16-plus-240. I don't trust it too much because is much better than any of the other models. 3.4 Summary We have achieved competitive scores using Openclip models. 3.5 Next steps Create an ensemble.","title":"Iteration 3. Train Logistic Regression on top of OpenClip"},{"location":"modeling/03_Iteration_3/#iteration-3-train-logistic-regression-on-top-of-openclip","text":"","title":"Iteration 3. Train Logistic Regression on top of OpenClip"},{"location":"modeling/03_Iteration_3/#31-goal","text":"We are going to train a Logistic Regression model on top of the OpenClip embeddings. OpenClip models are pretrained on LAION dataset which is the same dataset that was used to train Stable Diffusion. Stable diffusion is able to generate satellite images so that implies that there are satellite images on LAION dataset. Thus we can use the OpenClip embeddings to train a model that can classify satellite images.","title":"3.1 Goal"},{"location":"modeling/03_Iteration_3/#32-development","text":"I have prepared a script following the examples of the github repo If we had more time it would have been better to fine-tune the OpenClip model instead of training a Logistic Regression on top of the embeddings.","title":"3.2 Development"},{"location":"modeling/03_Iteration_3/#33-results","text":"model pretrained val f1 score ViT-B-16-plus-240 laion400m_e32 0.7837 ViT-B-32-quickgelu laion2b_s34b_b79k 0.743 ViT-L-14 laion2b_s32b_b82k 0.7335 ViT-L-14 laion400m_e32 0.7325 ViT-H-14 laion2b_s32b_b79k 0.7248 ViT-B-16 openai 0.7245 ViT-g-14 laion2b_s12b_b42k 0.7224 ViT-B-16 laion400m_e32 0.7193 ViT-B-32-quickgelu openai 0.715 ViT-L-14 openai 0.7084 ViT-B-32-quickgelu laion400m_e32 0.6966 The results are quite impressive for a simple Logistic Regression model, this implies that the features extracted by the OpenClip models are very useful for the task. Specially good is the result of ViT-B-16-plus-240. I don't trust it too much because is much better than any of the other models.","title":"3.3 Results"},{"location":"modeling/03_Iteration_3/#34-summary","text":"We have achieved competitive scores using Openclip models.","title":"3.4 Summary"},{"location":"modeling/03_Iteration_3/#35-next-steps","text":"Create an ensemble.","title":"3.5 Next steps"},{"location":"modeling/04_Iteration_4/","text":"Iteration 4. Create an ensemble 4.1 Goal We have the predictions from the previous iterations and we have to combine them to create the final solution. 4.2 Development Since there is little remaining time I'm going to follow a greedy strategy for choosing the models of the ensemble. I will be using the top n best models. 4.3 Results I don't have too much confident on the best model, the difference with the others is suspicious. Thus I prefer to use more models at the cost of a lower validation score with the hope that the ensemble will generalize better to the test set. Score of the ensemble depending on the number of models used: 1 models Val ensemble score:0.783743 2 models Val ensemble score:0.768413 3 models Val ensemble score:0.773294 4 models Val ensemble score:0.754268 5 models Val ensemble score:0.745553 6 models Val ensemble score:0.767213 7 models Val ensemble score:0.756134 8 models Val ensemble score:0.764636 9 models Val ensemble score:0.756578 10 models Val ensemble score:0.753637 11 models Val ensemble score:0.744580 12 models Val ensemble score:0.751459 13 models Val ensemble score:0.753778 14 models Val ensemble score:0.751143 15 models Val ensemble score:0.748809 16 models Val ensemble score:0.751143 17 models Val ensemble score:0.748809 18 models Val ensemble score:0.750816 19 models Val ensemble score:0.750816 20 models Val ensemble score:0.748809 21 models Val ensemble score:0.750816 22 models Val ensemble score:0.744487 23 models Val ensemble score:0.737434 24 models Val ensemble score:0.732917 25 models Val ensemble score:0.730574 26 models Val ensemble score:0.739550 27 models Val ensemble score:0.728364 28 models Val ensemble score:0.728364 29 models Val ensemble score:0.723910 I have decided to use the top 13 models. This results on a ensemble with a validation score of 0.754 This ensemble combines both Openclip models and keras pretrained models. Single model scores: score name 0 0.7837 ViT-B-16-plus-240_laion400m_e32 1 0.7430 ViT-B-32_laion2b_s34b_b79k 2 0.7391 ResNet50V2_2 3 0.7335 ViT-L-14_laion2b_s32b_b82k 4 0.7325 ViT-L-14_laion400m_e32 5 0.7281 ResNet50_1 6 0.7248 ViT-H-14_laion2b_s32b_b79k 7 0.7245 ViT-B-16_openai 8 0.7224 ViT-g-14_laion2b_s12b_b42k 9 0.7215 ResNet50V2_0 10 0.7193 ViT-B-16_laion400m_e32 11 0.7162 ResNet50_2 12 0.7154 MobileNetV2_2 4.4 Summary We have prepared an ensemble of models that we will use to make the final predictions. 4.5 Next steps Prepare the documentation for the challenge.","title":"Iteration 4. Create an ensemble"},{"location":"modeling/04_Iteration_4/#iteration-4-create-an-ensemble","text":"","title":"Iteration 4. Create an ensemble"},{"location":"modeling/04_Iteration_4/#41-goal","text":"We have the predictions from the previous iterations and we have to combine them to create the final solution.","title":"4.1 Goal"},{"location":"modeling/04_Iteration_4/#42-development","text":"Since there is little remaining time I'm going to follow a greedy strategy for choosing the models of the ensemble. I will be using the top n best models.","title":"4.2 Development"},{"location":"modeling/04_Iteration_4/#43-results","text":"I don't have too much confident on the best model, the difference with the others is suspicious. Thus I prefer to use more models at the cost of a lower validation score with the hope that the ensemble will generalize better to the test set. Score of the ensemble depending on the number of models used: 1 models Val ensemble score:0.783743 2 models Val ensemble score:0.768413 3 models Val ensemble score:0.773294 4 models Val ensemble score:0.754268 5 models Val ensemble score:0.745553 6 models Val ensemble score:0.767213 7 models Val ensemble score:0.756134 8 models Val ensemble score:0.764636 9 models Val ensemble score:0.756578 10 models Val ensemble score:0.753637 11 models Val ensemble score:0.744580 12 models Val ensemble score:0.751459 13 models Val ensemble score:0.753778 14 models Val ensemble score:0.751143 15 models Val ensemble score:0.748809 16 models Val ensemble score:0.751143 17 models Val ensemble score:0.748809 18 models Val ensemble score:0.750816 19 models Val ensemble score:0.750816 20 models Val ensemble score:0.748809 21 models Val ensemble score:0.750816 22 models Val ensemble score:0.744487 23 models Val ensemble score:0.737434 24 models Val ensemble score:0.732917 25 models Val ensemble score:0.730574 26 models Val ensemble score:0.739550 27 models Val ensemble score:0.728364 28 models Val ensemble score:0.728364 29 models Val ensemble score:0.723910 I have decided to use the top 13 models. This results on a ensemble with a validation score of 0.754 This ensemble combines both Openclip models and keras pretrained models. Single model scores: score name 0 0.7837 ViT-B-16-plus-240_laion400m_e32 1 0.7430 ViT-B-32_laion2b_s34b_b79k 2 0.7391 ResNet50V2_2 3 0.7335 ViT-L-14_laion2b_s32b_b82k 4 0.7325 ViT-L-14_laion400m_e32 5 0.7281 ResNet50_1 6 0.7248 ViT-H-14_laion2b_s32b_b79k 7 0.7245 ViT-B-16_openai 8 0.7224 ViT-g-14_laion2b_s12b_b42k 9 0.7215 ResNet50V2_0 10 0.7193 ViT-B-16_laion400m_e32 11 0.7162 ResNet50_2 12 0.7154 MobileNetV2_2","title":"4.3 Results"},{"location":"modeling/04_Iteration_4/#44-summary","text":"We have prepared an ensemble of models that we will use to make the final predictions.","title":"4.4 Summary"},{"location":"modeling/04_Iteration_4/#45-next-steps","text":"Prepare the documentation for the challenge.","title":"4.5 Next steps"},{"location":"utils/00_Challenge_Workflow/","text":"Challenge workflow Start of the challenge Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n barbol pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 conda activate barbol python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude and collaborate on the forum. Add a nice picture to README End of the challenge Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#challenge-workflow","text":"","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#start-of-the-challenge","text":"Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n barbol pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 conda activate barbol python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude and collaborate on the forum. Add a nice picture to README","title":"Start of the challenge"},{"location":"utils/00_Challenge_Workflow/#end-of-the-challenge","text":"Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"End of the challenge"},{"location":"utils/markdown_cheatsheet/","text":"Markdown cheatsheet Examples of attaching images First an image with markdown syntax Next an image with html syntax that allows to control the size Examples of equations Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) . Easy way to create tables http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#markdown-cheatsheet","text":"","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#examples-of-attaching-images","text":"First an image with markdown syntax Next an image with html syntax that allows to control the size","title":"Examples of attaching images"},{"location":"utils/markdown_cheatsheet/#examples-of-equations","text":"Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) .","title":"Examples of equations"},{"location":"utils/markdown_cheatsheet/#easy-way-to-create-tables","text":"http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Easy way to create tables"},{"location":"utils/methodology/","text":"Methodology I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on challenges. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"},{"location":"utils/methodology/#methodology","text":"I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on challenges. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"}]}